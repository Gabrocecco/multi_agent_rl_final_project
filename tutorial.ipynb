{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked-AI library, including loading and evaluating agents and visualizing trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4bad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 10/10 [00:01<00:00,  6.68it/s]\n",
      "Avg rew: 200.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301b7633156e4c04aa5d0b6164401b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.agent import AgentPair, RandomAgent\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "\n",
    "ap = AgentPair(RandomAgent(), RandomAgent())\n",
    "\n",
    "trajs = ae.evaluate_agent_pair(ap, 10)\n",
    "\n",
    "trajs2 = ae.evaluate_human_model_pair(1)\n",
    "\n",
    "\n",
    "StateVisualizer().display_rendered_trajectory(trajs2, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f70d1",
   "metadata": {},
   "source": [
    "# Deprecated stuff which requires BC and RL training (see README for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# Getting started: Training your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "You can train BC agents using files under the `human_aware_rl/imitation` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f493c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 12:58:47.198840: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-11 12:58:47.255350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746961127.311514   65144 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746961127.326921   65144 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746961127.368793   65144 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746961127.368864   65144 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746961127.368870   65144 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746961127.368875   65144 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-11 12:58:47.384319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/gabro/anaconda3/envs/overcooked-rl/lib/python3.10/site-packages/ray/tune/logger/tensorboardx.py:35: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  VALID_NP_HPARAMS = (np.bool8, np.float32, np.float64, np.int32, np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/gabro/Desktop/AAS/final_project/overcooked_ai_AAS_24-25/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle\n",
      "Number of trajectories processed for each layout: {'cramped_room': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 12:59:28.967783: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabro/anaconda3/envs/overcooked-rl/lib/python3.10/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: Overcooked_observation\n",
      "Received: inputs=['Tensor(shape=(None, 96))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 - 5s - 11ms/step - loss: 0.9522 - sparse_categorical_accuracy: 0.7189 - val_loss: 0.8915 - val_sparse_categorical_accuracy: 0.7058 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.8414 - sparse_categorical_accuracy: 0.7246 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.7058 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.8031 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7983 - val_sparse_categorical_accuracy: 0.7070 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.7864 - sparse_categorical_accuracy: 0.7224 - val_loss: 0.7836 - val_sparse_categorical_accuracy: 0.7024 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.7764 - sparse_categorical_accuracy: 0.7220 - val_loss: 0.7787 - val_sparse_categorical_accuracy: 0.7040 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "446/446 - 3s - 7ms/step - loss: 0.7692 - sparse_categorical_accuracy: 0.7209 - val_loss: 0.7755 - val_sparse_categorical_accuracy: 0.7040 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "446/446 - 3s - 7ms/step - loss: 0.7633 - sparse_categorical_accuracy: 0.7222 - val_loss: 0.7641 - val_sparse_categorical_accuracy: 0.7026 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.7583 - sparse_categorical_accuracy: 0.7239 - val_loss: 0.7643 - val_sparse_categorical_accuracy: 0.7024 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "446/446 - 3s - 6ms/step - loss: 0.7549 - sparse_categorical_accuracy: 0.7246 - val_loss: 0.7617 - val_sparse_categorical_accuracy: 0.7062 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "446/446 - 3s - 7ms/step - loss: 0.7512 - sparse_categorical_accuracy: 0.7240 - val_loss: 0.7673 - val_sparse_categorical_accuracy: 0.7044 - learning_rate: 0.0010\n",
      "Saving bc model at  tutorial_notebook_results/BC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Functional name=functional, built=True>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = \"cramped_room\" # any compatible layouts \n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import get_bc_params, train_bc_model\n",
    "from human_aware_rl.static import CLEAN_2019_HUMAN_DATA_TRAIN\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"tutorial_notebook_results/BC\", bc_params, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.1) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94ab2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional, built=True>,\n",
       " {'eager': True,\n",
       "  'use_lstm': False,\n",
       "  'cell_size': 256,\n",
       "  'data_params': {'layouts': ['cramped_room'],\n",
       "   'check_trajectories': False,\n",
       "   'featurize_states': True,\n",
       "   'data_path': '/home/gabro/Desktop/AAS/final_project/overcooked_ai_AAS_24-25/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle'},\n",
       "  'mdp_params': {'layout_name': 'cramped_room', 'old_dynamics': True},\n",
       "  'env_params': {'horizon': 400,\n",
       "   'mlam_params': {'start_orientations': False,\n",
       "    'wait_allowed': False,\n",
       "    'counter_goals': [],\n",
       "    'counter_drop': [],\n",
       "    'counter_pickup': [],\n",
       "    'same_motion_goals': True}},\n",
       "  'mdp_fn_params': {},\n",
       "  'mlp_params': {'num_layers': 2, 'net_arch': [64, 64]},\n",
       "  'training_params': {'epochs': 10,\n",
       "   'validation_split': 0.15,\n",
       "   'batch_size': 64,\n",
       "   'learning_rate': 0.001,\n",
       "   'use_class_weights': False},\n",
       "  'evaluation_params': {'ep_length': 400, 'num_games': 1, 'display': False},\n",
       "  'action_shape': (6,),\n",
       "  'observation_shape': (96,)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"tutorial_notebook_results/BC\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c37a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7eb81138c730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(bc_model, bc_params, stochastic=True)\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent0 = RlLibAgent(bc_policy, 0, base_env.featurize_state_mdp)\n",
    "bc_agent0\n",
    "\n",
    "bc_agent1 = RlLibAgent(bc_policy, 1, base_env.featurize_state_mdp)\n",
    "bc_agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7eb80f35b790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap_bc = AgentPair(bc_agent0, bc_agent1)\n",
    "ap_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95787dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.benchmarking.AgentEvaluator at 0x7eb80f35ae00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93676beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 56.00 (std: 17.44, se: 5.51); avg len: 400.00; : 100%|██████████| 10/10 [13:39<00:00, 81.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ep_rewards': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 20, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=object),\n",
       " 'mdp_params': array([{'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]}],\n",
       "       dtype=object),\n",
       " 'ep_actions': array([[((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, -1), (0, 0)), ...,\n",
       "         ((-1, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 1)), ('interact', 'interact'), ((0, 0), (0, 0))],\n",
       "        ...,\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (1, 0))],\n",
       "        [((0, 0), (1, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ('interact', (1, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ('interact', (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))]],\n",
       "       dtype=object),\n",
       " 'ep_infos': array([[{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.12858465, 0.00141527, 0.02543445, 0.03501986, 0.78705865,\n",
       "                 0.02248717]], dtype=float32)}, {'action_probs': array([[0.05082351, 0.0174262 , 0.00641956, 0.00217616, 0.88881654,\n",
       "                 0.03433806]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.09638028, 0.00163328, 0.00985803, 0.05270959, 0.8354414 ,\n",
       "                 0.00397746]], dtype=float32)}, {'action_probs': array([[0.05149857, 0.0184186 , 0.00465547, 0.00164694, 0.8977245 ,\n",
       "                 0.02605591]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.09638028, 0.00163328, 0.00985803, 0.05270959, 0.8354414 ,\n",
       "                 0.00397746]], dtype=float32)}, {'action_probs': array([[0.05149857, 0.0184186 , 0.00465547, 0.00164694, 0.8977245 ,\n",
       "                 0.02605591]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[11], [27, 53, 109, 126, 140, 200, 222, 241]], 'useful_onion_pickup': [[11], [27, 53, 109, 126, 140, 200, 222, 241]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[17], [41, 59, 119, 134, 149, 213, 227, 257]], 'dish_pickup': [[23, 144, 220], [297]], 'useful_dish_pickup': [[23, 144, 220], []], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[80, 176, 285, 366], []], 'soup_delivery': [[91, 316], []], 'soup_drop': [[187, 367], []], 'optimal_onion_potting': [[17], [41, 59, 119, 134, 149, 213, 227, 257]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[17], [41, 59, 119, 134, 149, 213, 227, 257]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([40,  0]), 'cumulative_shaped_rewards_by_agent': array([27, 24])}, 'ep_sparse_r': 40, 'ep_shaped_r': 51, 'ep_sparse_r_by_agent': array([40,  0]), 'ep_shaped_r_by_agent': array([27, 24]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.0367577 , 0.01247025, 0.03130547, 0.00366578, 0.9112452 ,\n",
       "                 0.0045556 ]], dtype=float32)}, {'action_probs': array([[0.03902743, 0.00860376, 0.04607902, 0.01709239, 0.6402253 ,\n",
       "                 0.24897215]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0367577 , 0.01247025, 0.03130547, 0.00366578, 0.9112452 ,\n",
       "                 0.0045556 ]], dtype=float32)}, {'action_probs': array([[0.03902743, 0.00860376, 0.04607902, 0.01709239, 0.6402253 ,\n",
       "                 0.24897215]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0367577 , 0.01247025, 0.03130547, 0.00366578, 0.9112452 ,\n",
       "                 0.0045556 ]], dtype=float32)}, {'action_probs': array([[0.03902743, 0.00860376, 0.04607902, 0.01709239, 0.6402253 ,\n",
       "                 0.24897215]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[10, 27, 104, 129, 138, 159, 192, 205], [23, 43, 268, 301, 387]], 'useful_onion_pickup': [[10, 27, 104, 129, 138, 159, 192, 205], [23, 43, 268, 301, 387]], 'onion_drop': [[], [58, 312]], 'useful_onion_drop': [[], []], 'potting_onion': [[22, 63, 110, 136, 153, 182, 203, 208], [37, 289, 393]], 'dish_pickup': [[300], [73, 129, 201]], 'useful_dish_pickup': [[300], [73, 129, 201]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[], [98, 173, 230]], 'soup_delivery': [[], [114, 183, 237]], 'soup_drop': [[], []], 'optimal_onion_potting': [[22, 63, 110, 136, 153, 182, 203, 208], [37, 289, 393]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[22, 63, 110, 136, 153, 182, 203, 208], [37, 289, 393]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([ 0, 60]), 'cumulative_shaped_rewards_by_agent': array([27, 33])}, 'ep_sparse_r': 60, 'ep_shaped_r': 60, 'ep_sparse_r_by_agent': array([ 0, 60]), 'ep_shaped_r_by_agent': array([27, 33]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.02724497, 0.00717345, 0.05710855, 0.33710486, 0.53587854,\n",
       "                 0.03548969]], dtype=float32)}, {'action_probs': array([[0.00348352, 0.22701497, 0.05603497, 0.00267163, 0.66704583,\n",
       "                 0.04374909]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05609503, 0.00804172, 0.10113126, 0.24904378, 0.5467379 ,\n",
       "                 0.0389503 ]], dtype=float32)}, {'action_probs': array([[0.00093728, 0.04360382, 0.00711449, 0.00920234, 0.5094554 ,\n",
       "                 0.4296867 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 20], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.04442925, 0.01011779, 0.08592405, 0.1611858 , 0.6838498 ,\n",
       "                 0.01449324]], dtype=float32)}, {'action_probs': array([[0.03897287, 0.00846241, 0.0104228 , 0.31157303, 0.62562805,\n",
       "                 0.00494079]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[146, 252, 261, 276, 325, 356], [36, 52, 60, 135, 154, 176]], 'useful_onion_pickup': [[146, 252, 261, 276, 325, 356], [36, 52, 60, 135, 154, 176]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[158, 256, 272, 321, 341, 364], [48, 57, 90, 144, 167, 243]], 'dish_pickup': [[29, 169], [115, 256, 313]], 'useful_dish_pickup': [[], []], 'dish_drop': [[], [130]], 'useful_dish_drop': [[], [130]], 'soup_pickup': [[124, 203], [293, 386]], 'soup_delivery': [[136, 214], [305, 398]], 'soup_drop': [[], []], 'optimal_onion_potting': [[158, 256, 272, 321, 341, 364], [48, 57, 90, 144, 167, 243]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[158, 256, 272, 321, 341, 364], [48, 57, 90, 144, 167, 243]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([40, 40]), 'cumulative_shaped_rewards_by_agent': array([28, 28])}, 'ep_sparse_r': 80, 'ep_shaped_r': 56, 'ep_sparse_r_by_agent': array([40, 40]), 'ep_shaped_r_by_agent': array([28, 28]), 'ep_length': 400}}],\n",
       "        ...,\n",
       "        [{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.07121462, 0.00639487, 0.02646123, 0.03117403, 0.74721384,\n",
       "                 0.11754146]], dtype=float32)}, {'action_probs': array([[5.4471403e-02, 3.8118814e-03, 2.0168881e-01, 2.4173062e-03,\n",
       "                 7.3707753e-01, 5.3310371e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.07121462, 0.00639487, 0.02646123, 0.03117403, 0.74721384,\n",
       "                 0.11754146]], dtype=float32)}, {'action_probs': array([[5.4471403e-02, 3.8118814e-03, 2.0168881e-01, 2.4173062e-03,\n",
       "                 7.3707753e-01, 5.3310371e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.07121462, 0.00639487, 0.02646123, 0.03117403, 0.74721384,\n",
       "                 0.11754146]], dtype=float32)}, {'action_probs': array([[5.4471403e-02, 3.8118814e-03, 2.0168881e-01, 2.4173062e-03,\n",
       "                 7.3707753e-01, 5.3310371e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[14, 26, 65, 239, 263, 286, 333, 351], [19, 34, 103, 192, 196, 345]], 'useful_onion_pickup': [[14, 26, 239, 263, 286, 333, 351], [19, 34, 345]], 'onion_drop': [[], [100, 183, 195, 198]], 'useful_onion_drop': [[], [100, 183, 195, 198]], 'potting_onion': [[24, 48, 234, 259, 276, 327, 340], [30, 354]], 'dish_pickup': [[], [206, 282, 364, 384]], 'useful_dish_pickup': [[], [206, 282, 364]], 'dish_drop': [[], [379]], 'useful_dish_drop': [[], []], 'soup_pickup': [[], [217, 311]], 'soup_delivery': [[], [236, 318]], 'soup_drop': [[], []], 'optimal_onion_potting': [[24, 48, 234, 259, 276, 327, 340], [30, 354]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[24, 48, 234, 259, 276, 327, 340], [30, 354]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([ 0, 40]), 'cumulative_shaped_rewards_by_agent': array([21, 25])}, 'ep_sparse_r': 40, 'ep_shaped_r': 46, 'ep_sparse_r_by_agent': array([ 0, 40]), 'ep_shaped_r_by_agent': array([21, 25]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02420625, 0.00632518, 0.0115635 , 0.01534877, 0.9359219 ,\n",
       "                 0.00663448]], dtype=float32)}, {'action_probs': array([[0.01074767, 0.00255955, 0.05388125, 0.00418494, 0.86998665,\n",
       "                 0.05863998]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02420625, 0.00632518, 0.0115635 , 0.01534877, 0.9359219 ,\n",
       "                 0.00663448]], dtype=float32)}, {'action_probs': array([[0.01074767, 0.00255955, 0.05388125, 0.00418494, 0.86998665,\n",
       "                 0.05863998]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[5.3067380e-03, 4.1719907e-04, 1.4683952e-03, 2.6755711e-02,\n",
       "                 9.5662498e-01, 9.4269821e-03]], dtype=float32)}, {'action_probs': array([[0.09192687, 0.00282741, 0.13248819, 0.00376242, 0.76275504,\n",
       "                 0.00624005]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None}        ,\n",
       "         {'agent_infos': [{'action_probs': array([[4.1260333e-03, 6.6983589e-04, 1.8165507e-03, 7.2659333e-03,\n",
       "                 9.7450012e-01, 1.1621624e-02]], dtype=float32)}, {'action_probs': array([[0.30989593, 0.0048513 , 0.00798421, 0.00364265, 0.6706898 ,\n",
       "                 0.00293607]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None}        ,\n",
       "         {'agent_infos': [{'action_probs': array([[4.1260333e-03, 6.6983589e-04, 1.8165507e-03, 7.2659333e-03,\n",
       "                 9.7450012e-01, 1.1621624e-02]], dtype=float32)}, {'action_probs': array([[0.30989593, 0.0048513 , 0.00798421, 0.00364265, 0.6706898 ,\n",
       "                 0.00293607]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[23, 52, 104, 112, 349, 391], [12, 23, 227, 348]], 'useful_onion_pickup': [[23, 52, 104, 112, 349], [12, 23, 227, 348]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[47, 98, 107, 130, 378], [20, 30, 343, 368]], 'dish_pickup': [[134], [42, 108, 394]], 'useful_dish_pickup': [[], [42, 108]], 'dish_drop': [[322], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[], [77, 155]], 'soup_delivery': [[], [88, 160]], 'soup_drop': [[], []], 'optimal_onion_potting': [[47, 98, 107, 130, 378], [20, 30, 343, 368]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[47, 98, 107, 130, 378], [20, 30, 343, 368]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([ 0, 40]), 'cumulative_shaped_rewards_by_agent': array([15, 28])}, 'ep_sparse_r': 40, 'ep_shaped_r': 43, 'ep_sparse_r_by_agent': array([ 0, 40]), 'ep_shaped_r_by_agent': array([15, 28]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03787744, 0.00629121, 0.01445258, 0.01788438, 0.9180131 ,\n",
       "                 0.00548127]], dtype=float32)}, {'action_probs': array([[0.01930079, 0.00154835, 0.02292833, 0.00913279, 0.9421011 ,\n",
       "                 0.00498857]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.01853419, 0.02477049, 0.01303868, 0.07019582, 0.7128904 ,\n",
       "                 0.1605705 ]], dtype=float32)}, {'action_probs': array([[0.01274243, 0.00225298, 0.00665095, 0.28722918, 0.67696434,\n",
       "                 0.01416006]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.01142775, 0.00736427, 0.16388619, 0.00644597, 0.7996901 ,\n",
       "                 0.01118571]], dtype=float32)}, {'action_probs': array([[0.0213984 , 0.00336916, 0.01027837, 0.26701498, 0.6861261 ,\n",
       "                 0.01181297]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.01142775, 0.00736427, 0.16388619, 0.00644597, 0.7996901 ,\n",
       "                 0.01118571]], dtype=float32)}, {'action_probs': array([[0.0213984 , 0.00336916, 0.01027837, 0.26701498, 0.6861261 ,\n",
       "                 0.01181297]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[24, 42, 232, 397], [12, 35, 222, 258, 305, 324, 367]], 'useful_onion_pickup': [[24, 42, 232, 397], [12, 35, 222, 258, 305, 324, 367]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[35, 215, 263], [27, 48, 238, 299, 312, 349]], 'dish_pickup': [[275, 305, 358], [96]], 'useful_dish_pickup': [[275, 305], [96]], 'dish_drop': [[332], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[289, 380], [197]], 'soup_delivery': [[295, 384], [206]], 'soup_drop': [[], []], 'optimal_onion_potting': [[35, 215, 263], [27, 48, 238, 299, 312, 349]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[35, 215, 263], [27, 48, 238, 299, 312, 349]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([40, 20]), 'cumulative_shaped_rewards_by_agent': array([25, 26])}, 'ep_sparse_r': 60, 'ep_shaped_r': 51, 'ep_sparse_r_by_agent': array([40, 20]), 'ep_shaped_r_by_agent': array([25, 26]), 'ep_length': 400}}]],\n",
       "       dtype=object),\n",
       " 'env_params': array([{'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1}],\n",
       "       dtype=object),\n",
       " 'ep_returns': array([40, 60, 80, 40, 40, 80, 80, 40, 40, 60]),\n",
       " 'metadatas': {},\n",
       " 'ep_states': array([[<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb80f35b190>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77fffc2b0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77fae1e40>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77fbf1e70>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77fbf0550>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f897c70>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f896a70>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb80f35bc10>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77fae3460>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f824ca0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f48c790>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f7767a0>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f7ff9d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f9926b0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f991a20>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f99b7f0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f656d40>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f9988b0>],\n",
       "        ...,\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ebb8be0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f732f50>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb7800d5d20>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f291000>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f22dd20>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f25b910>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f1a1690>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ee40730>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ea98d90>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77e9a2320>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ef3ff10>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77e9b1300>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ef3e410>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f3e2290>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77f3c4d30>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ec12ec0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ec47a00>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7eb77ee1c7c0>]],\n",
       "       dtype=object),\n",
       " 'ep_dones': array([[False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True]], dtype=object),\n",
       " 'ep_lengths': array([400, 400, 400, 400, 400, 400, 400, 400, 400, 400])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ap: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "trajs = ae.evaluate_agent_pair(ap_bc, 10)\n",
    "trajs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fed7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 60, 80, 40, 40, 80, 80, 40, 40, 60])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs[\"ep_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48875a68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap_sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m ae\u001b[38;5;241m.\u001b[39mevaluate_agent_pair(\u001b[43map_sp\u001b[49m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m400\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ap_sp' is not defined"
     ]
    }
   ],
   "source": [
    "result = ae.evaluate_agent_pair(ap_sp, 1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464d0c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a125b9c0b8e4396afd1ff2cca36ccb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b62122",
   "metadata": {},
   "source": [
    "This should spawn a window where you can see what the agents are doing at each timestep. You can drag the slider to go forward and backward in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overcooked-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
